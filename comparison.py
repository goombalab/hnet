import numpy as np
import json
import torch
from omegaconf import ListConfig
from dataclasses import dataclass

from hnet.models.mixer_seq import HNetForCausalLM # for comparison purposes
from hnet_simple import HNetLM, AttnConfig, SSMConfig, HNetConfig


@dataclass(frozen=True)
class ByteTokenizer:
    vocab_size: int = 256
    eos_idx: int = 255
    bos_idx: int = 254
    bos: bytes = b'\xfe'
    def encode(self, seqs: list[str]): return [
        { 'input_ids': np.frombuffer(self.bos+s.encode(), dtype=np.uint8) }
        for s in seqs
    ]
    def decode(self, tokens, **k) -> str:
        if isinstance(tokens, np.ndarray): tokens = tokens.tolist()
        return bytearray(tokens).decode("utf-8", **k)


###
### Model loading

def load_config(config_path: str) -> HNetConfig:
    with open(config_path, "r") as f:
        c = json.load(f)
        attn_cfg = AttnConfig(**c.pop("attn_cfg"))
        ssm_cfg = SSMConfig(**c.pop("ssm_cfg"))
        return HNetConfig(**c, attn_cfg=attn_cfg, ssm_cfg=ssm_cfg)

def load_models(model_path: str, config_path: str, *, device: str='cuda'):
    c = load_config(config_path)

    with torch.device(device):
        model_old = HNetForCausalLM(c, device=device, dtype=torch.bfloat16).eval()
        model_new = HNetLM(c).bfloat16().eval()

    with torch.serialization.safe_globals([ListConfig]):
        state_dict = torch.load(model_path, map_location=device, weights_only=False)
        model_old.load_state_dict(state_dict)
        model_new.load_state_dict(state_dict)

    return model_old, model_new


###
### Generation fns

@torch.inference_mode
def generate(model, prompt: str, max_tokens: int = 1024, temperature: float = 1.0, top_p: float = 0.9):
    device = next(model.parameters()).device

    # Tokenize prompt
    tokenizer = ByteTokenizer()
    iids = tokenizer.encode([prompt])[0]['input_ids']
    iids = torch.tensor(iids, dtype=torch.long, device=device)[None]

    # prefill
    inference_cache = model.allocate_inference_cache(1, iids.size(1)+max_tokens, dtype=torch.bfloat16)
    with torch.inference_mode():
        mask = torch.ones(iids.shape, device=device, dtype=torch.bool)
        output = model(iids, mask=mask, inference_params=inference_cache)

    logits = getattr(output,'logits',output)[0, -1, :] / temperature

    s_0 = getattr(inference_cache,'get_innermost_seqlen',lambda:-1)()
    for _ in range(max_tokens):
        # Apply top-p sampling
        if top_p < 1.0:
            sorted_logits, sorted_indices = torch.sort(logits, descending=True)
            cumulative_probs = torch.cumsum(
                torch.softmax(sorted_logits, dim=-1), dim=-1
            )

            # Remove tokens with cumulative probability above the threshold
            sorted_indices_to_remove = cumulative_probs > top_p
            sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
            sorted_indices_to_remove[0] = 0

            indices_to_remove = sorted_indices[sorted_indices_to_remove]
            logits[indices_to_remove] = -float("inf")

        probs = torch.softmax(logits, dim=-1)
        next_token = torch.multinomial(probs, 1)

        if next_token.item() in [tokenizer.eos_idx, tokenizer.bos_idx]:
            break

        current_token = next_token.unsqueeze(0)
        s_1 = getattr(inference_cache,'get_innermost_seqlen',lambda:-1)()
        yield (current_token, s_1>s_0)
        s_0 = s_1

        with torch.inference_mode():
            output = model.step(current_token, inference_cache)

        # Get logits and apply temperature
        logits = getattr(output,'logits',output)[0, -1, :] / temperature

def yield_utf8_chunks(g):
    '''Aggregate raw generated bytes into utf-8 bytes,
    *and* further aggregate them by when the model chunks them.

    Note that, if model chunking boundaries are between UTF-8 codepoints,
    this generator will simply produce wrong results.
    '''
    tokenizer = ByteTokenizer()
    chars = []
    chunk = []
    hadChunkStart = False
    for c,chunkStart in g:
        hadChunkStart |= chunkStart
        if hadChunkStart and chunk:
            yield ''.join(chunk)
            chunk = []
            hadChunkStart = False

        chars.append(c.item())
        try:
            chunk.append(tokenizer.decode(chars))
            chars = []
        except: # only let UDC raise up iff exceeds max byte length
            if len(chars) > 4: raise

    if chunk: yield ''.join(chunk)
    if chars: pass # do nothing to dropped bytes


###
### Equivalence checking

def compare_numerics(m_old, m_new, max_tokens):
    __import__("lovely_tensors").monkey_patch()

    x = torch.tensor(bytearray(b'\xfeTesting, Testing'), dtype=torch.long, device='cuda')[None]
    c_old = m_old.allocate_inference_cache(1, x.shape[-1]+max_tokens, dtype=torch.bfloat16)
    c_new = m_old.allocate_inference_cache(1, x.shape[-1]+max_tokens, dtype=torch.bfloat16)

    # prefill
    with torch.inference_mode():
        o_old = m_old.forward(x, torch.ones_like(x,dtype=torch.bool), inference_params=c_old).logits
        o_new = m_new.forward(x, c_new)
    print(o_old)
    print(o_new)
    print('prefill diff:', o_old-o_new)

    # decode
    next_token = o_old[:,-1:].argmax(-1)
    with torch.inference_mode():
        t_old = m_old.step(next_token, c_old).logits
        t_new = m_new.step(next_token, c_new)
    print(t_old)
    print(t_new)
    print(' decode diff:', t_old-t_new)

def compare_generate(m_old, m_new, tokenizer, *, p='hello world'):
    def fullgen(*a,**k): return [t[0] for t in generate(*a,max_tokens=50,**k)]
    # compare generation
    y_old = fullgen(m_old, p, temperature=0.001, top_p=0.001)
    y_new = fullgen(m_new, p, temperature=0.001, top_p=0.001)
    assert y_old == y_new

    # compare random consistency
    torch.manual_seed(0); y_old = fullgen(m_old, p)
    torch.manual_seed(0); y_new = fullgen(m_new, p)
    assert y_old == y_new
    print('generation:', repr(tokenizer.decode(bytearray(torch.cat(y_old)[:,-1]))))


###
### typer CLI

def main(
    model: str='hnet_1stage_L.pt', config: str='configs/hnet_1stage_L.json',
    *,
    temp: float=1.0, top_p: float=1.0, max_tokens: int=1024,
    validation: bool = False,
):
    tokenizer = ByteTokenizer()
    print("Loading model...")
    m_old, m_new = load_models(model, config)

    if validation:
        compare_numerics(m_old, m_new, max_tokens)
        compare_generate(m_old, m_new, tokenizer)
        return

    print("Warming up...")
    list(generate(m_new, 'hello world!!!', max_tokens, temp, top_p))

    while True:
        prompt = input("\nPrompt: ").strip()
        if not prompt: exit()
        print(f"\nGenerating ({max_tokens=}, {temp=}, {top_p=})")

        from termcolor import colored
        print(colored(prompt, 'green'), end='')

        # alternate between white-on-black and black-on-white (not very accurate)
        for i,chunk in enumerate(yield_utf8_chunks(
            generate(m_new, prompt, max_tokens, temp, top_p)
        )):
            s = '\n'.join(
                colored(c, 'white' if i%2 else 'black', 'on_black' if i%2 else 'on_white')
                for c in chunk.split('\n')
            )
            print(s, flush=True, end='')
        print()


# To validate: `LOVELY_TENSORS=1 uv run comparison.py`
if __name__ == '__main__': main(validation=True)
# To test prompting: `PYTHONPATH=. _TYPER_STANDARD_TRACEBACK=1 uv run typer comparison.py run`
