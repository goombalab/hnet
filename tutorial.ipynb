{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8b74a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "hnet 0.0.1 requires causal_conv1d@ git+https://github.com/Dao-AILab/causal-conv1d.git@e940ead2fd962c56854455017541384909ca669f, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install huggingface_hub[hf_transfer] regex transformers ipywidgets omegaconf optree -U --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764b6849",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U pip setuptools packaging wheel ninja pybind11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e92ce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install flash-attention for arm64\n",
    "# git clone https://github.com/johnnynunez/flash-attention/tree/main\n",
    "# git submodule update --init --recursive\n",
    "# https://github.com/Dao-AILab/flash-attention/pull/1507\n",
    "FLASH_ATTENTION_TRITON_AMD_ENABLE=TRUE MAX_JOBS=50 pip install flash-attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2144e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_HUB_ENABLE_HF_TRANSFER=1\n"
     ]
    }
   ],
   "source": [
    "%env HF_HUB_ENABLE_HF_TRANSFER=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "699bcaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check mamba installation\n",
    "try:\n",
    "    import torch\n",
    "    from mamba_ssm import Mamba\n",
    "\n",
    "    batch, length, dim = 2, 64, 16\n",
    "    x = torch.randn(batch, length, dim).to(\"cuda\")\n",
    "    model = Mamba(\n",
    "        # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "        d_model=dim, # Model dimension d_model\n",
    "        d_state=16,  # SSM state expansion factor\n",
    "        d_conv=4,    # Local convolution width\n",
    "        expand=2,    # Block expansion factor\n",
    "    ).to(\"cuda\")\n",
    "    y = model(x)\n",
    "    assert y.shape == x.shape\n",
    "except Exception as e:\n",
    "    print(f\"Error during Mamba import or execution: {e}\")\n",
    "    print(\"Please ensure that mamba_ssm is installed and configured correctly.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1da1ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading files from cartesia-ai/hnet_2stage_XL to ./data...\n",
      "/home/ubuntu/miniconda3/envs/torch/lib/python3.11/site-packages/huggingface_hub/file_download.py:980: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n",
      ".gitattributes: 1.52kB [00:00, 9.73MB/s]\n",
      "hnet_2stage_XL.pt: 100%|██████████████████▉| 6.41G/6.41G [00:05<00:00, 1.10GB/s]\n",
      "Download complete.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} download.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f51e7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hnet_2stage_XL.pt\n"
     ]
    }
   ],
   "source": [
    "!ls ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc7be01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded successfully!\n",
      "\n",
      "Generating (max_tokens=128, temperature=1.0, top_p=1.0)\n",
      "\u001b[92mIn a shocking finding, scientist discovered a herd of unicorns\u001b[0m"
     ]
    }
   ],
   "source": [
    "!{sys.executable} generate.py \\\n",
    "    --model-path ./data/hnet_2stage_XL.pt \\\n",
    "    --config-path configs/hnet_2stage_XL.json \\\n",
    "    --max-tokens 128 \\\n",
    "    --temperature 1.0 \\\n",
    "    --top-p 1.0 \\\n",
    "    --prompt \"In a shocking finding, scientist discovered a herd of unicorns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61943c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
